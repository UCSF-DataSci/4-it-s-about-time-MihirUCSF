{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Advanced Analysis\n",
    "\n",
    "In this part, we will implement advanced analysis techniques for physiological time series data, including time-domain feature extraction, frequency analysis, and wavelet transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "import pywt\n",
    "import os\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn')\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Time-Domain Feature Extraction\n",
    "\n",
    "Implement the `extract_time_domain_features` function to extract various time-domain features from physiological signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_domain_features(data, window_size=60):\n",
    "    # convert window_size from seconds to number of samples\n",
    "    # assuming data is sampled at 1 Hz (1 sample per second)\n",
    "    window_samples = window_size\n",
    "    \n",
    "    # initialize DataFrame for features\n",
    "    features = pd.DataFrame()\n",
    "    features['timestamp'] = data['timestamp']\n",
    "    features['subject_id'] = data['subject_id']\n",
    "    features['session'] = data['session']\n",
    "    \n",
    "    # process each physiological signal\n",
    "    for signal_name in ['heart_rate', 'eda', 'temperature']:\n",
    "        if signal_name in data.columns:\n",
    "            # basic statistics using rolling window\n",
    "            features[f'{signal_name}_mean'] = data[signal_name].rolling(window=window_samples).mean()\n",
    "            features[f'{signal_name}_std'] = data[signal_name].rolling(window=window_samples).std()\n",
    "            features[f'{signal_name}_min'] = data[signal_name].rolling(window=window_samples).min()\n",
    "            features[f'{signal_name}_max'] = data[signal_name].rolling(window=window_samples).max()\n",
    "    \n",
    "    # heart rate variability measures\n",
    "    if 'heart_rate' in data.columns:\n",
    "        # convert HR to RR intervals in milliseconds (ms)\n",
    "        rr_intervals = 60000 / data['heart_rate']\n",
    "        # calculate successive differences between RR intervals\n",
    "        rr_diff = rr_intervals.diff()\n",
    "        # RMSSD (Root Mean Square of Successive Differences) in ms\n",
    "        features['hrv_rmssd_ms'] = np.sqrt(\n",
    "            rr_diff.abs().rolling(window=window_samples).apply(\n",
    "                lambda x: np.mean(x**2)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # SDNN (Standard Deviation of NN intervals) in ms\n",
    "        features['hrv_sdnn_ms'] = rr_intervals.rolling(window=window_samples).std()\n",
    "        # pNN50 (Percentage of successive RR intervals differing by >50ms)\n",
    "        features['hrv_pnn50_percent'] = rr_diff.rolling(window=window_samples).apply(\n",
    "            lambda x: 100 * np.sum(np.abs(x) > 50) / len(x) if len(x) > 0 else 0\n",
    "        )\n",
    "    \n",
    "    # EDA-specific features if available\n",
    "    if 'eda' in data.columns:\n",
    "        # skin conductance level (tonic component)\n",
    "        features['eda_tonic_uS'] = data['eda'].rolling(window=window_samples).mean()\n",
    "        \n",
    "        # skin conductance response frequency (number of significant rises)\n",
    "        # define a threshold for significant EDA change (typically 0.05 µS)\n",
    "        eda_diff = data['eda'].diff()\n",
    "        features['eda_response_freq_per_min'] = eda_diff.rolling(window=window_samples).apply(\n",
    "            lambda x: np.sum(x > 0.05) * (60 / window_size)  # Convert to per minute\n",
    "        )\n",
    "    \n",
    "    # temperature-specific features if available\n",
    "    if 'temperature' in data.columns:\n",
    "        # temperature rate of change (°C/min)\n",
    "        features['temp_change_rate_C_per_min'] = data['temperature'].diff().rolling(window=window_samples).mean() * (60 / window_size)\n",
    "    \n",
    "    features = features.dropna()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Domain Feature Data Test\n",
    "# load sample data for testing\n",
    "from pathlib import Path\n",
    "\n",
    "# check if processed data exists\n",
    "processed_files = list(Path('data/processed').glob('*.csv'))\n",
    "subject_files = [f for f in processed_files if not f.name.startswith('all_')]\n",
    "\n",
    "if subject_files:\n",
    "    print(f\"Found {len(subject_files)} subject data files\")\n",
    "    \n",
    "    # create a directory for individual feature plots\n",
    "    os.makedirs('plots/time_features', exist_ok=True)\n",
    "    \n",
    "    # dictionary to store features for all subjects\n",
    "    all_features = {}\n",
    "    \n",
    "    # process each subject file\n",
    "    for sample_file in subject_files:\n",
    "        subject_id = sample_file.stem.split('_')[0] \n",
    "        session = '_'.join(sample_file.stem.split('_')[1:-1])  \n",
    "        \n",
    "        print(f\"\\nProcessing: {subject_id}, {session}\")\n",
    "        \n",
    "        sample_data = pd.read_csv(sample_file)\n",
    "        \n",
    "        # convert timestamp to datetime\n",
    "        sample_data['timestamp'] = pd.to_datetime(sample_data['timestamp'])\n",
    "        # extract features\n",
    "        time_features = extract_time_domain_features(sample_data, window_size=60)\n",
    "        # store in dictionary\n",
    "        all_features[f\"{subject_id}_{session}\"] = time_features\n",
    "        # display basic info\n",
    "        print(f\"  Extracted {time_features.shape[0]} feature windows\")\n",
    "        # create a visualization for this subject\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # heart rate plot with variability\n",
    "        plt.subplot(3, 1, 1)\n",
    "        plt.plot(time_features['timestamp'], time_features['heart_rate_mean'], 'b-', label='Mean HR')\n",
    "        plt.fill_between(\n",
    "            time_features['timestamp'],\n",
    "            time_features['heart_rate_mean'] - time_features['heart_rate_std'],\n",
    "            time_features['heart_rate_mean'] + time_features['heart_rate_std'],\n",
    "            alpha=0.2, color='b'\n",
    "        )\n",
    "        plt.title(f'Heart Rate with Standard Deviation - {subject_id}, {session}')\n",
    "        plt.ylabel('BPM')\n",
    "        plt.legend()\n",
    "        \n",
    "        # HRV measures\n",
    "        plt.subplot(3, 1, 2)\n",
    "        plt.plot(time_features['timestamp'], time_features['hrv_rmssd_ms'], 'r-', label='RMSSD')\n",
    "        plt.plot(time_features['timestamp'], time_features['hrv_sdnn_ms'], 'g-', label='SDNN')\n",
    "        plt.title('Heart Rate Variability Measures')\n",
    "        plt.ylabel('ms')\n",
    "        plt.legend()\n",
    "        \n",
    "        # EDA with response frequency\n",
    "        plt.subplot(3, 1, 3)\n",
    "        plt.plot(time_features['timestamp'], time_features['eda_tonic_uS'], 'b-', label='Tonic EDA')\n",
    "        plt.plot(time_features['timestamp'], time_features['eda_response_freq_per_min'], 'm-', label='EDA Responses/min')\n",
    "        plt.title('Electrodermal Activity Features')\n",
    "        plt.xlabel('Time')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/time_features/{subject_id}_{session}_time_features.png', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # combined feature dataset\n",
    "    combined_features = pd.concat(all_features.values(), ignore_index=True)\n",
    "    combined_features.to_csv('data/processed/all_time_features.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nCombined features dataset created with shape: {combined_features.shape}\")\n",
    "    print(\"Saved to: data/processed/all_time_features.csv\")\n",
    "    \n",
    "else:\n",
    "    print(\"No processed data files found. Please run part1_exploration.ipynb first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Frequency Analysis\n",
    "\n",
    "Implement the `analyze_frequency_components` function to perform frequency-domain analysis on the signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_frequency_components(data, sampling_rate, window_size=60, subject_id=None, session=None, output_dir='plots/frequency'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # convert window_size from seconds to number of samples\n",
    "    window_samples = int(window_size * sampling_rate)\n",
    "    results = {}\n",
    "    \n",
    "    # process physiological signals\n",
    "    for signal_name in ['heart_rate', 'eda', 'temperature']:\n",
    "        if signal_name not in data.columns:\n",
    "            continue\n",
    "            \n",
    "        signal_results = {}\n",
    "        \n",
    "        # process data in windows\n",
    "        n_windows = max(1, len(data) // window_samples)\n",
    "        all_frequencies = []\n",
    "        all_power = []\n",
    "        \n",
    "        for i in range(n_windows):\n",
    "            start_idx = i * window_samples\n",
    "            end_idx = min((i + 1) * window_samples, len(data))\n",
    "            \n",
    "            window_data = data[signal_name].iloc[start_idx:end_idx]\n",
    "            \n",
    "            # skip windows with not enough data\n",
    "            if len(window_data) < window_samples // 2:\n",
    "                continue\n",
    "                \n",
    "            # check for NaN or inf values and clean the data\n",
    "            if np.isnan(window_data).any() or np.isinf(window_data).any():\n",
    "                # replace infinities with NaNs \n",
    "                window_data = window_data.replace([np.inf, -np.inf], np.nan)\n",
    "                nan_ratio = window_data.isna().mean()\n",
    "                if nan_ratio > 0.5:  \n",
    "                    print(f\"  Skipping window with {nan_ratio*100:.1f}% missing data\")\n",
    "                    continue\n",
    "                    \n",
    "                # interpolate any remaining NaN values\n",
    "                window_data = window_data.interpolate(method='linear').fillna(method='ffill').fillna(method='bfill')\n",
    "                \n",
    "                if np.isnan(window_data).any() or np.isinf(window_data).any():\n",
    "                    print(f\"  Skipping window with remaining NaN/Inf values\")\n",
    "                    continue\n",
    "            \n",
    "            # detrend the data to remove linear trends\n",
    "            try:\n",
    "                window_data_detrended = signal.detrend(window_data.values)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error during detrending: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # apply Hann window to reduce spectral leakage\n",
    "            window_data_windowed = window_data_detrended * signal.windows.hann(len(window_data_detrended))\n",
    "            \n",
    "            # calculate PSD using Welch's method\n",
    "            try:\n",
    "                frequencies, power = signal.welch(\n",
    "                    window_data_windowed,\n",
    "                    fs=sampling_rate,\n",
    "                    nperseg=min(256, len(window_data_windowed)),\n",
    "                    noverlap=min(128, len(window_data_windowed) // 2),\n",
    "                    scaling='density'\n",
    "                )\n",
    "                \n",
    "                all_frequencies.append(frequencies)\n",
    "                all_power.append(power)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error in Welch's method: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # skip signals with no valid windows\n",
    "        if not all_frequencies:\n",
    "            print(f\"  No valid windows for {signal_name}\")\n",
    "            continue\n",
    "            \n",
    "        # calculate average results\n",
    "        signal_results['frequencies'] = np.mean(all_frequencies, axis=0)\n",
    "        signal_results['power'] = np.mean(all_power, axis=0)\n",
    "        \n",
    "        # frequency bands\n",
    "        bands = {\n",
    "            'VLF': (0.003, 0.04),  \n",
    "            'LF': (0.04, 0.15),    \n",
    "            'HF': (0.15, 0.4)      \n",
    "        }\n",
    "        \n",
    "        # calculate power in each band\n",
    "        signal_results['bands'] = {}\n",
    "        for band_name, (low, high) in bands.items():\n",
    "            mask = (signal_results['frequencies'] >= low) & (signal_results['frequencies'] <= high)\n",
    "            if not any(mask):\n",
    "                signal_results['bands'][band_name] = 0\n",
    "                continue\n",
    "            \n",
    "            signal_results['bands'][band_name] = np.sum(signal_results['power'][mask])\n",
    "        \n",
    "        # calculate LF/HF ratio\n",
    "        if signal_results['bands']['HF'] > 0:\n",
    "            signal_results['bands']['LF/HF'] = signal_results['bands']['LF'] / signal_results['bands']['HF']\n",
    "        else:\n",
    "            signal_results['bands']['LF/HF'] = np.nan\n",
    "        \n",
    "        # create a plot of the power spectrum\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # plot the power spectrum\n",
    "        plt.semilogy(signal_results['frequencies'], signal_results['power'], 'b-')\n",
    "        \n",
    "        # highlight frequency bands\n",
    "        colors = {'VLF': 'green', 'LF': 'orange', 'HF': 'red'}\n",
    "        for band_name, (low, high) in bands.items():\n",
    "            mask = (signal_results['frequencies'] >= low) & (signal_results['frequencies'] <= high)\n",
    "            if any(mask):\n",
    "                plt.fill_between(\n",
    "                    signal_results['frequencies'][mask], \n",
    "                    signal_results['power'][mask], \n",
    "                    alpha=0.3, \n",
    "                    color=colors[band_name],\n",
    "                    label=f\"{band_name}: {low:.3f}-{high:.3f} Hz\"\n",
    "                )\n",
    "        \n",
    "        # add labels and title\n",
    "        plt.xlabel('Frequency (Hz)')\n",
    "        plt.ylabel('Power Spectral Density')\n",
    "        \n",
    "        title = f'Power Spectrum of {signal_name.replace(\"_\", \" \").title()}'\n",
    "        if subject_id and session:\n",
    "            title += f' - Subject {subject_id}, {session}'\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.grid(True, which='both', linestyle='--', alpha=0.5)\n",
    "        plt.legend()\n",
    "        \n",
    "        if subject_id and session:\n",
    "            plot_filename = f\"{subject_id}_{session}_{signal_name}_fft.png\"\n",
    "        else:\n",
    "            plot_filename = f\"{signal_name}_fft.png\"\n",
    "        \n",
    "        plt.savefig(os.path.join(output_dir, plot_filename), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        if subject_id and session:\n",
    "            data_filename = f\"{subject_id}_{session}_{signal_name}_fft\"\n",
    "        else:\n",
    "            data_filename = f\"{signal_name}_fft\"\n",
    "        \n",
    "        np.savez(\n",
    "            os.path.join(output_dir, f\"{data_filename}.npz\"),\n",
    "            frequencies=signal_results['frequencies'],\n",
    "            power=signal_results['power'],\n",
    "            vlf_power=signal_results['bands']['VLF'],\n",
    "            lf_power=signal_results['bands']['LF'],\n",
    "            hf_power=signal_results['bands']['HF'],\n",
    "            lf_hf_ratio=signal_results['bands']['LF/HF']\n",
    "        )\n",
    "        \n",
    "        freq_df = pd.DataFrame({\n",
    "            'frequency': signal_results['frequencies'],\n",
    "            'power': signal_results['power']\n",
    "        })\n",
    "        freq_df.to_csv(os.path.join(output_dir, f\"{data_filename}.csv\"), index=False)\n",
    "        \n",
    "        results[signal_name] = signal_results\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Analysis Data Test\n",
    "freq_output_dir = 'plots/frequency'\n",
    "os.makedirs(freq_output_dir, exist_ok=True)\n",
    "\n",
    "processed_files = list(Path('data/processed').glob('*.csv'))\n",
    "subject_files = [f for f in processed_files if not f.name.startswith('all_')]\n",
    "\n",
    "if subject_files:\n",
    "    print(f\"Found {len(subject_files)} subject files for frequency analysis\")\n",
    "    all_freq_results = {}\n",
    "    \n",
    "    for sample_file in subject_files:\n",
    "        try:\n",
    "            subject_id = sample_file.stem.split('_')[0]\n",
    "            session = '_'.join(sample_file.stem.split('_')[1:-1])\n",
    "            \n",
    "            print(f\"\\nPerforming frequency analysis for Subject {subject_id}, Session {session}\")\n",
    "            \n",
    "            # load the data\n",
    "            sample_data = pd.read_csv(sample_file)\n",
    "            \n",
    "            # convert timestamp to datetime\n",
    "            sample_data['timestamp'] = pd.to_datetime(sample_data['timestamp'])\n",
    "            \n",
    "            # check for and report missing data\n",
    "            missing_data = sample_data[['heart_rate', 'eda', 'temperature']].isna().sum()\n",
    "            if missing_data.sum() > 0:\n",
    "                print(f\"  Warning: Dataset contains missing values:\\n{missing_data}\")\n",
    "            \n",
    "            # check for infinite values\n",
    "            inf_data = np.isinf(sample_data[['heart_rate', 'eda', 'temperature']]).sum()\n",
    "            if inf_data.sum() > 0:\n",
    "                print(f\"  Warning: Dataset contains infinite values:\\n{inf_data}\")\n",
    "                \n",
    "                # clean infinities\n",
    "                sample_data = sample_data.replace([np.inf, -np.inf], np.nan)\n",
    "                sample_data = sample_data.interpolate(method='linear')\n",
    "                sample_data = sample_data.fillna(method='ffill').fillna(method='bfill')\n",
    "                \n",
    "                print(\"  Replaced infinite values with interpolated values\")\n",
    "            \n",
    "            # determine sampling rate based on timestamps\n",
    "            time_diff = np.median(np.diff(sample_data['timestamp'].astype(int))) / 1e9\n",
    "            sampling_rate = 1.0 / time_diff\n",
    "            \n",
    "            print(f\"  Detected sampling rate: {sampling_rate:.2f} Hz\")\n",
    "            \n",
    "            # frequency analysis\n",
    "            freq_results = analyze_frequency_components(\n",
    "                sample_data,\n",
    "                sampling_rate,\n",
    "                window_size=60,\n",
    "                subject_id=subject_id,\n",
    "                session=session,\n",
    "                output_dir=freq_output_dir\n",
    "            )\n",
    "            \n",
    "            if not freq_results:\n",
    "                print(f\"  No valid results for {subject_id}, {session}\")\n",
    "                continue\n",
    "                \n",
    "            all_freq_results[f\"{subject_id}_{session}\"] = freq_results\n",
    "            \n",
    "            # key results\n",
    "            for signal_name, results in freq_results.items():\n",
    "                print(f\"\\n  {signal_name.upper()} frequency analysis:\")\n",
    "                print(f\"    VLF power: {results['bands']['VLF']:.4f}\")\n",
    "                print(f\"    LF power: {results['bands']['LF']:.4f}\")\n",
    "                print(f\"    HF power: {results['bands']['HF']:.4f}\")\n",
    "                print(f\"    LF/HF ratio: {results['bands']['LF/HF']:.4f}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {sample_file.name}: {e}\")\n",
    "    \n",
    "    # continue with summary table if there are results\n",
    "    if all_freq_results:\n",
    "        # summary table of LF/HF ratios (a key stress indicator)\n",
    "        lf_hf_summary = []\n",
    "        for subject_session, freq_results in all_freq_results.items():\n",
    "            subject_id, session = subject_session.split('_', 1)\n",
    "            \n",
    "            for signal_name, results in freq_results.items():\n",
    "                lf_hf_summary.append({\n",
    "                    'subject_id': subject_id,\n",
    "                    'session': session,\n",
    "                    'signal': signal_name,\n",
    "                    'VLF_power': results['bands']['VLF'],\n",
    "                    'LF_power': results['bands']['LF'],\n",
    "                    'HF_power': results['bands']['HF'],\n",
    "                    'LF_HF_ratio': results['bands']['LF/HF']\n",
    "                })\n",
    "        \n",
    "        lf_hf_df = pd.DataFrame(lf_hf_summary)\n",
    "        lf_hf_df.to_csv(os.path.join(freq_output_dir, 'frequency_analysis_summary.csv'), index=False)\n",
    "        \n",
    "        print(\"\\nFrequency analysis complete!\")\n",
    "        print(f\"Results saved to {freq_output_dir}\")\n",
    "        print(f\"Summary file: {os.path.join(freq_output_dir, 'frequency_analysis_summary.csv')}\")\n",
    "        \n",
    "        # comparison plot of LF/HF ratios across all subjects and sessions\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # filter for heart rate only\n",
    "        hr_data = lf_hf_df[lf_hf_df['signal'] == 'heart_rate']\n",
    "        \n",
    "        # plot LF/HF ratios by subject and session\n",
    "        ax = sns.barplot(x='subject_id', y='LF_HF_ratio', hue='session', data=hr_data)\n",
    "        \n",
    "        plt.title('Heart Rate LF/HF Ratio by Subject and Session')\n",
    "        plt.xlabel('Subject ID')\n",
    "        plt.ylabel('LF/HF Ratio (higher values indicate more stress)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(title='Session')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.savefig(os.path.join(freq_output_dir, 'lf_hf_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "else:\n",
    "    print(\"No processed data files found. Please run part1_exploration.ipynb first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Time-Frequency Analysis\n",
    "\n",
    "Implement the `analyze_time_frequency_features` function to analyze time-frequency features using wavelet transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_time_frequency_features(data, sampling_rate, window_size=60, signal_name='heart_rate', subject_id=None, session=None, output_dir='plots/wavelet'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # convert window_size from seconds to number of samples\n",
    "    window_samples = int(window_size * sampling_rate)\n",
    "    results = {}\n",
    "    \n",
    "    # check if signal exists in data\n",
    "    if signal_name not in data.columns:\n",
    "        print(f\"Signal {signal_name} not found in data\")\n",
    "        return results\n",
    "        \n",
    "    # replace NaN and infinity values\n",
    "    signal_data = data[signal_name].copy()\n",
    "    signal_data = signal_data.replace([np.inf, -np.inf], np.nan)\n",
    "    nan_ratio = signal_data.isna().mean()\n",
    "    if nan_ratio > 0.5:\n",
    "        print(f\"Too many missing values in {signal_name}: {nan_ratio*100:.1f}%\")\n",
    "        return results\n",
    "        \n",
    "    # interpolate missing values\n",
    "    signal_data = signal_data.interpolate(method='linear').ffill().bfill()\n",
    "    if signal_data.isna().any():\n",
    "        print(f\"Could not interpolate all missing values in {signal_name}\")\n",
    "        return results\n",
    "    \n",
    "    # define wavelet scales\n",
    "    scales = np.logspace(0, np.log10(128), num=64)\n",
    "    results['scales'] = scales\n",
    "    # convert scales to frequencies for interpretation\n",
    "    frequencies = pywt.scale2frequency('morl', scales) * sampling_rate\n",
    "    results['frequencies'] = frequencies\n",
    "    # process data in windows\n",
    "    n_windows = max(1, len(signal_data) // window_samples)\n",
    "    all_coefficients = []\n",
    "    all_energy = []\n",
    "    dominant_freqs = []\n",
    "    times = []\n",
    "    \n",
    "    for i in range(n_windows):\n",
    "        start_idx = i * window_samples\n",
    "        end_idx = min((i + 1) * window_samples, len(signal_data))\n",
    "        window_data = signal_data.iloc[start_idx:end_idx].values\n",
    "        \n",
    "        if len(window_data) < window_samples // 2:\n",
    "            continue\n",
    "        try:\n",
    "            coefficients, _ = pywt.cwt(\n",
    "                window_data,\n",
    "                scales,\n",
    "                'morl',\n",
    "                sampling_period=1/sampling_rate\n",
    "            )\n",
    "            \n",
    "            # calculate energy distribution\n",
    "            energy = np.abs(coefficients)**2\n",
    "            \n",
    "            # find dominant frequency at each time point\n",
    "            dominant_scale_idx = np.argmax(energy, axis=0)\n",
    "            dominant_freq = frequencies[dominant_scale_idx]\n",
    "            \n",
    "            all_coefficients.append(coefficients)\n",
    "            all_energy.append(energy)\n",
    "            dominant_freqs.append(dominant_freq)\n",
    "            \n",
    "            # time information for plotting\n",
    "            window_times = data['timestamp'].iloc[start_idx:end_idx]\n",
    "            times.append(window_times)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in wavelet transform: {e}\")\n",
    "            continue\n",
    "\n",
    "    if len(all_coefficients) == 0:\n",
    "        print(f\"No valid windows for wavelet analysis\")\n",
    "        return results\n",
    "    \n",
    "    # average results across windows\n",
    "    results['coefficients'] = np.mean(all_coefficients, axis=0)\n",
    "    results['time_frequency_energy'] = np.mean(all_energy, axis=0)\n",
    "    results['dominant_frequency'] = np.concatenate(dominant_freqs) if dominant_freqs else np.array([])\n",
    "    \n",
    "    # flatten times for plotting\n",
    "    flattened_times = pd.concat(times) if times else pd.Series()\n",
    "    \n",
    "    # visualization\n",
    "    if len(all_energy) > 0:\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # time-frequency plot (scalogram)\n",
    "        plt.subplot(211)\n",
    "        T, S = np.meshgrid(\n",
    "            np.arange(results['time_frequency_energy'].shape[1]), \n",
    "            np.arange(results['time_frequency_energy'].shape[0])\n",
    "        )\n",
    "        \n",
    "        plt.contourf(T, S, results['time_frequency_energy'], 100, cmap='jet')\n",
    "        plt.ylabel('Scale')\n",
    "        plt.title(f'Wavelet Scalogram - {signal_name}')\n",
    "        \n",
    "        plt.colorbar(label='Energy')\n",
    "        \n",
    "        ax1 = plt.gca()\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.set_yticks(np.arange(0, len(scales), len(scales)//5))\n",
    "        ax2.set_yticklabels([f\"{frequencies[i]:.2f}\" for i in range(0, len(scales), len(scales)//5)])\n",
    "        ax2.set_ylabel('Frequency (Hz)')\n",
    "        \n",
    "        # plot dominant frequency over time\n",
    "        plt.subplot(212)\n",
    "        \n",
    "        # concatenated dominant frequencies\n",
    "        if len(results['dominant_frequency']) > 0:\n",
    "            plt.plot(np.arange(len(results['dominant_frequency'])), results['dominant_frequency'], 'r-')\n",
    "            plt.xlabel('Time (samples)')\n",
    "            plt.ylabel('Dominant Frequency (Hz)')\n",
    "            plt.title('Dominant Frequency Components Over Time')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if subject_id and session:\n",
    "            fig_name = f\"{subject_id}_{session}_{signal_name}_wavelet.png\"\n",
    "        else:\n",
    "            fig_name = f\"{signal_name}_wavelet.png\"\n",
    "        \n",
    "        plt.savefig(os.path.join(output_dir, fig_name), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        if subject_id and session:\n",
    "            data_name = f\"{subject_id}_{session}_{signal_name}_wavelet\"\n",
    "        else:\n",
    "            data_name = f\"{signal_name}_wavelet\"\n",
    "        \n",
    "        np.savez(\n",
    "            os.path.join(output_dir, f\"{data_name}.npz\"),\n",
    "            scales=results['scales'],\n",
    "            frequencies=results['frequencies'],\n",
    "            coefficients=results['coefficients'],\n",
    "            energy=results['time_frequency_energy'],\n",
    "            dominant_frequency=results['dominant_frequency']\n",
    "        )\n",
    "        \n",
    "        energy_df = pd.DataFrame(\n",
    "            results['time_frequency_energy'],\n",
    "            index=[f\"scale_{i}\" for i in range(len(scales))],\n",
    "            columns=[f\"time_{i}\" for i in range(results['time_frequency_energy'].shape[1])]\n",
    "        )\n",
    "        energy_df.to_csv(os.path.join(output_dir, f\"{data_name}_energy.csv\"))\n",
    "        \n",
    "        if len(results['dominant_frequency']) > 0:\n",
    "            dom_freq_df = pd.DataFrame({\n",
    "                'time': np.arange(len(results['dominant_frequency'])),\n",
    "                'dominant_frequency': results['dominant_frequency']\n",
    "            })\n",
    "            dom_freq_df.to_csv(os.path.join(output_dir, f\"{data_name}_dominant_freq.csv\"), index=False)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Frequency Analysis Data Test \n",
    "wavelet_output_dir = 'plots/wavelet'\n",
    "os.makedirs(wavelet_output_dir, exist_ok=True)\n",
    "\n",
    "processed_files = list(Path('data/processed').glob('*.csv'))\n",
    "subject_files = [f for f in processed_files if not f.name.startswith('all_')]\n",
    "\n",
    "if subject_files:\n",
    "    print(f\"Found {len(subject_files)} subject files for wavelet analysis\")\n",
    "    all_wavelet_results = {}\n",
    "    \n",
    "    # process each subject file\n",
    "    for sample_file in subject_files:\n",
    "        try:\n",
    "            subject_id = sample_file.stem.split('_')[0]\n",
    "            session = '_'.join(sample_file.stem.split('_')[1:-1])\n",
    "            \n",
    "            print(f\"\\nPerforming wavelet analysis for Subject {subject_id}, Session {session}\")\n",
    "            \n",
    "            sample_data = pd.read_csv(sample_file)\n",
    "            \n",
    "            # convert timestamp to datetime\n",
    "            sample_data['timestamp'] = pd.to_datetime(sample_data['timestamp'])\n",
    "            \n",
    "            # determine sampling rate based on timestamps\n",
    "            time_diff = np.median(np.diff(sample_data['timestamp'].astype(int))) / 1e9\n",
    "            sampling_rate = 1.0 / time_diff\n",
    "            \n",
    "            print(f\"  Detected sampling rate: {sampling_rate:.2f} Hz\")\n",
    "            \n",
    "            # perform wavelet analysis on different signals\n",
    "            signals_to_analyze = ['heart_rate', 'eda', 'temperature']\n",
    "            \n",
    "            for signal_name in signals_to_analyze:\n",
    "                if signal_name not in sample_data.columns:\n",
    "                    print(f\"  Signal {signal_name} not found in data\")\n",
    "                    continue\n",
    "                    \n",
    "                print(f\"  Analyzing {signal_name} signal...\")\n",
    "                \n",
    "                # wavelet analysis\n",
    "                wavelet_results = analyze_time_frequency_features(\n",
    "                    sample_data,\n",
    "                    sampling_rate,\n",
    "                    window_size=60,\n",
    "                    signal_name=signal_name,\n",
    "                    subject_id=subject_id,\n",
    "                    session=session,\n",
    "                    output_dir=wavelet_output_dir\n",
    "                )\n",
    "                \n",
    "                if wavelet_results:\n",
    "                    result_key = f\"{subject_id}_{session}_{signal_name}\"\n",
    "                    all_wavelet_results[result_key] = wavelet_results\n",
    "                    print(f\"  ✓ Wavelet analysis complete for {signal_name}\")\n",
    "                else:\n",
    "                    print(f\"  ✗ Wavelet analysis failed for {signal_name}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {sample_file.name}: {e}\")\n",
    "    \n",
    "    # results summary\n",
    "    wavelet_summary = []\n",
    "    \n",
    "    for result_key, wavelet_result in all_wavelet_results.items():\n",
    "        if 'time_frequency_energy' in wavelet_result:\n",
    "            # extract subject, session, signal from the key\n",
    "            parts = result_key.split('_')\n",
    "            subject_id = parts[0]\n",
    "            signal = parts[-1]\n",
    "            session = '_'.join(parts[1:-1])\n",
    "            \n",
    "            # summary metrics\n",
    "            energy_matrix = wavelet_result['time_frequency_energy']\n",
    "            total_energy = np.sum(energy_matrix)\n",
    "            max_energy = np.max(energy_matrix)\n",
    "            frequency_with_max_energy = wavelet_result['frequencies'][np.argmax(np.sum(energy_matrix, axis=1))]\n",
    "            \n",
    "            wavelet_summary.append({\n",
    "                'subject_id': subject_id,\n",
    "                'session': session,\n",
    "                'signal': signal,\n",
    "                'total_energy': total_energy,\n",
    "                'max_energy': max_energy,\n",
    "                'frequency_with_max_energy': frequency_with_max_energy\n",
    "            })\n",
    "    \n",
    "    if wavelet_summary:\n",
    "        summary_df = pd.DataFrame(wavelet_summary)\n",
    "        summary_file = os.path.join(wavelet_output_dir, 'wavelet_analysis_summary.csv')\n",
    "        summary_df.to_csv(summary_file, index=False)\n",
    "        \n",
    "        print(\"\\nWavelet analysis complete!\")\n",
    "        print(f\"Results saved to {wavelet_output_dir}\")\n",
    "        print(f\"Summary file: {summary_file}\")\n",
    "        \n",
    "        # comparison plot of dominant frequencies for heart rate\n",
    "        hr_summary = summary_df[summary_df['signal'] == 'heart_rate']\n",
    "        \n",
    "        if not hr_summary.empty:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            ax = sns.barplot(\n",
    "                x='subject_id', \n",
    "                y='frequency_with_max_energy', \n",
    "                hue='session', \n",
    "                data=hr_summary\n",
    "            )\n",
    "            \n",
    "            plt.title('Dominant Frequency of Heart Rate by Subject and Session')\n",
    "            plt.xlabel('Subject ID')\n",
    "            plt.ylabel('Frequency (Hz)')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.legend(title='Session')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            comparison_file = os.path.join(wavelet_output_dir, 'dominant_frequency_comparison.png')\n",
    "            plt.savefig(comparison_file, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"Comparison plot saved as: {comparison_file}\")\n",
    "    else:\n",
    "        print(\"No wavelet results were generated\")\n",
    "        \n",
    "else:\n",
    "    print(\"No processed data files found. Please run part1_exploration.ipynb first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Here's how to use these functions with your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "data = pd.read_csv('data/processed/S1_processed.csv')\n",
    "\n",
    "# Extract time-domain features\n",
    "features = extract_time_domain_features(data, window_size=60)\n",
    "print(\"Time-domain features:\")\n",
    "print(features.head())\n",
    "\n",
    "# Analyze frequency components\n",
    "sampling_rate = 4.0  # Hz\n",
    "freq_results = analyze_frequency_components(data, sampling_rate, window_size=60)\n",
    "print(\"\\nFrequency analysis results:\")\n",
    "print(\"Frequency bands:\", freq_results['bands'])\n",
    "\n",
    "# Analyze time-frequency features\n",
    "tf_results = analyze_time_frequency_features(data, sampling_rate, window_size=60)\n",
    "print(\"\\nTime-frequency analysis results:\")\n",
    "print(\"Wavelet scales:\", tf_results['scales'].shape)\n",
    "print(\"Coefficients shape:\", tf_results['coefficients'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
